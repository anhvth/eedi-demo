{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates synthetic data generation using LLMs (GPT-4, DeepSeek, Qwen) via the DSPy library. The process involves:\n",
    "\n",
    "1. Selecting relevant bootstrap examples\n",
    "2. Using LLMs to generate additional data\n",
    "3. Filtering out irrelevant examples\n",
    "4. Using final curated data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-06 01:05:30.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meedi.data.common\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1mcommon: ['VectorDBRetriever', 'prepare_db_train_val', 'preproc_df', 'get_df_parsed', 'format_df_generic', 'compute_metric', 'mine_hard_negatives', 'mine_hard_negatives', 'extract_tag', 'TEMPLATE_INPUT_V3', 'df_mapping']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "# Load environment variables, openai api key\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import dspy\n",
    "from speedy_utils.all import *\n",
    "from eedi.data.common import *\n",
    "\n",
    "lm = dspy.LM(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate synthetic data\n",
    "\n",
    "For implementation details, see:\n",
    "- `data/retriver/01_generate_plan.ipynb`\n",
    "- `data/retriver/02_generate_synthetic_data.ipynb`\n",
    "\n",
    "Note: Generated data is provided in the `data` folder for direct use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and preprocess training data\n",
    "\n",
    "The training data is composed of the competition's provided data and the generated synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val = pd.read_csv(\"./data/train.csv\")\n",
    "# split fold\n",
    "df_train = df_train_val[df_train_val.QuestionId % 5 != 0].copy()\n",
    "df_val = df_train_val[df_train_val.QuestionId % 5 == 0].copy()\n",
    "\n",
    "df_miscon = pd.read_csv(\"./data/misconception_mapping.csv\")\n",
    "df_train_flat = preproc_df(df_train, df_miscon, is_train=True)\n",
    "df_train_flat = df_train_flat.dropna(subset=[\"MisconceptionName\"])\n",
    "df_val_flat = preproc_df(df_val, df_miscon, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data curation\n",
    "The synthedic data might contain low-quality examples. We will filter out these examples using a prompt-based filtering approach.\n",
    "- Filter low-quality examples.\n",
    "    - Framework: dspy\n",
    "    - Method: prompt-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "class DataCuratorSignature(dspy.Signature):\n",
    "    \"\"\"Analyze how well an incorrect answer reflects a suspected misconception in a mathematics problem.\n",
    "    The goal is to determine whether there is a clear, logical connection between the misconception and the wrong answer.\n",
    "\n",
    "    You task:\n",
    "    - Analyze the problem data and the suspected misconception\n",
    "       - Explain misconception->wrong answer connection\n",
    "       - Score (0-10):\n",
    "         10: Perfect alignment\n",
    "         8-9: Strong alignment\n",
    "         5-7: Moderate alignment\n",
    "         1-4: Weak alignment\n",
    "         0: No alignment\n",
    "\n",
    "    Guidelines:\n",
    "    - Focus on misconception-answer connection\n",
    "    - Be specific about error path\n",
    "    - Flag assumptions\n",
    "    - Consider consistency\n",
    "    \"\"\"\n",
    "\n",
    "    problem_data = dspy.InputField(description=\"Problem data\")\n",
    "    scratchpad: str = dspy.OutputField(\n",
    "        description=\"Analysis of the problem and misconception\"\n",
    "    )\n",
    "    evaluation_score: int = dspy.OutputField(\n",
    "        description=\"Evaluation explanation and score (0-10)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_prompt_for_curate(query, pos):\n",
    "    # use query and pos[0] to fill in the template\n",
    "    PROBLEM_DATA = \"{}\\nSuspected misconception: {}\".format(query, pos)\n",
    "    return PROBLEM_DATA\n",
    "\n",
    "curator_program = dspy.Predict(DataCuratorSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multi-thread, Function: `run_c..: 100%|██████████████████████████████| 2587/2587 [00:06<00:00, 413.69it/s, SUCCESS=2587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi thread results:\n",
      "+---------+---------+\n",
      "| Key     |   Value |\n",
      "+=========+=========+\n",
      "| SUCCESS |    2587 |\n",
      "+---------+---------+\n",
      "Drop low quality data: 332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load synthetic data and format prompts\n",
    "\n",
    "df_synthetic = pd.read_parquet(\"./data/synthetic_data.parquet\")\n",
    "df_synthetic[\"PROMPT\"] = df_synthetic.apply(\n",
    "    lambda x: TEMPLATE_INPUT_V3.format(**x), axis=1\n",
    ")\n",
    "\n",
    "def run_curator_on_synthetic_data(row):\n",
    "    prompt = format_prompt_for_curate(row[\"PROMPT\"], row[\"MISCONCEPTION_NAME\"])\n",
    "    output = curator_program(problem_data=prompt, lm=lm)\n",
    "    return output.evaluation_score\n",
    "\n",
    "df_synthetic[\"EVALUATION_SCORE\"] = multi_thread(run_curator_on_synthetic_data, df_synthetic, workers=128)\n",
    "\n",
    "# Keep only high-quality synthetic data\n",
    "print('Drop low quality data:', (df_synthetic[\"EVALUATION_SCORE\"] < 8).sum())\n",
    "\n",
    "df_synthetic = df_synthetic[df_synthetic[\"EVALUATION_SCORE\"] >= 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-06T01:05:41.159336]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `problem_data` (str): Problem data\n",
      "\n",
      "Your output fields are:\n",
      "1. `scratchpad` (str): Analysis of the problem and misconception\n",
      "2. `evaluation_score` (int): Evaluation explanation and score (0-10)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## problem_data ## ]]\n",
      "{problem_data}\n",
      "\n",
      "[[ ## scratchpad ## ]]\n",
      "{scratchpad}\n",
      "\n",
      "[[ ## evaluation_score ## ]]\n",
      "{evaluation_score}        # note: the value you produce must be a single int value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Analyze how well an incorrect answer reflects a suspected misconception in a mathematics problem.\n",
      "        The goal is to determine whether there is a clear, logical connection between the misconception and the wrong answer.\n",
      "        \n",
      "        You task:\n",
      "        - Analyze the problem data and the suspected misconception\n",
      "           - Explain misconception->wrong answer connection\n",
      "           - Score (0-10):\n",
      "             10: Perfect alignment\n",
      "             8-9: Strong alignment\n",
      "             5-7: Moderate alignment\n",
      "             1-4: Weak alignment\n",
      "             0: No alignment\n",
      "        \n",
      "        Guidelines:\n",
      "        - Focus on misconception-answer connection\n",
      "        - Be specific about error path\n",
      "        - Flag assumptions\n",
      "        - Consider consistency\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## problem_data ## ]]\n",
      "Question: What is the equation of this line? ![The diagram shows a pair of axes, x and y ranging from -6 to 6 horizontally and -6 to 6 vertically. There is a vertical line that passes through the x axis at 3. ]()\n",
      "    \n",
      "SubjectName: Horizontal and Vertical Lines\n",
      "ConstructName: Recognise the equations of vertical lines from a diagram\n",
      "Correct text: \\( x=3 \\)\n",
      "Student wrong answer: \\( y=3 \\)\n",
      "Suspected misconception: Thinks that in the equation x=a, a is the gradient\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## scratchpad ## ]]`, then `[[ ## evaluation_score ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## scratchpad ## ]]\n",
      "The student's answer of \\( y=3 \\) indicates a misunderstanding of how vertical lines are represented in coordinate geometry. The correct equation for a vertical line is \\( x=a \\), where \\( a \\) is the x-coordinate through which the line passes. The misconception that the student has—believing that in the equation \\( x=a \\), \\( a \\) represents the gradient—leads them to incorrectly assume that the line is horizontal, which is represented by \\( y=b \\) for some constant \\( b \\). This misunderstanding directly connects to their wrong answer, as they have confused the representation of vertical and horizontal lines. The error path shows a clear misconception about the nature of vertical lines and their equations, leading to the incorrect conclusion that the line's equation should be in the form of \\( y=3 \\).\n",
      "\n",
      "[[ ## evaluation_score ## ]]\n",
      "8\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some examples to check the quality of the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data examples: TOTAL: 3498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>MisconceptionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- **Question**: Simplify the following, if pos...</td>\n",
       "      <td>Does not know that to factorise a quadratic ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- **Question**: Simplify the following, if pos...</td>\n",
       "      <td>Thinks that when you cancel identical terms fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- **Question**: Simplify the following, if pos...</td>\n",
       "      <td>Does not know that to factorise a quadratic ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0  - **Question**: Simplify the following, if pos...   \n",
       "1  - **Question**: Simplify the following, if pos...   \n",
       "2  - **Question**: Simplify the following, if pos...   \n",
       "\n",
       "                                   MisconceptionName  \n",
       "0  Does not know that to factorise a quadratic ex...  \n",
       "1  Thinks that when you cancel identical terms fr...  \n",
       "2  Does not know that to factorise a quadratic ex...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data examples: TOTAL: 2255\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROMPT</th>\n",
       "      <th>MISCONCEPTION_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: In a triangle, the measures of the a...</td>\n",
       "      <td>Does not know that angles in a triangle sum to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: Calculate:\\n\\(\\n\\frac{2}{3} \\times \\...</td>\n",
       "      <td>Uses dividing fractions method for multiplying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: What is the sum of all angles around...</td>\n",
       "      <td>Believes there are 100 degrees in a full turn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              PROMPT  \\\n",
       "0  Question: In a triangle, the measures of the a...   \n",
       "1  Question: Calculate:\\n\\(\\n\\frac{2}{3} \\times \\...   \n",
       "2  Question: What is the sum of all angles around...   \n",
       "\n",
       "                                  MISCONCEPTION_NAME  \n",
       "0  Does not know that angles in a triangle sum to...  \n",
       "1  Uses dividing fractions method for multiplying...  \n",
       "2      Believes there are 100 degrees in a full turn  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"Train data examples:\", \"TOTAL:\", len(df_train_flat))\n",
    "display(df_train_flat.head(3)[[\"Prompt\", \"MisconceptionName\"]])\n",
    "print(\"Synthetic data examples:\", \"TOTAL:\", len(df_synthetic))\n",
    "\n",
    "display(df_synthetic.head(3)[[\"PROMPT\", \"MISCONCEPTION_NAME\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize training data\n",
    "The final training data is a combination of the original training data and the high-quality synthetic data.\n",
    "Each item is come with query and pos (misconception name) for the data curator to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for training the retriever... total: 5753\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "queries = df_train_flat[\"Prompt\"].tolist() + df_synthetic[\"PROMPT\"].tolist()\n",
    "misconceptions = (\n",
    "    df_train_flat[\"MisconceptionName\"].tolist()\n",
    "    + df_synthetic[\"MISCONCEPTION_NAME\"].tolist()\n",
    ")\n",
    "for query, misconception in zip(queries, misconceptions):\n",
    "    item = {\n",
    "        \"query\": query,\n",
    "        \"pos\": [misconception],\n",
    "    }\n",
    "    items.append(item)\n",
    "print(\"Saving data for training the retriever...\", 'total:', len(items))\n",
    "\n",
    "dump_json_or_pickle(items, \"./data/retriver/train_items.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num misconceptions: 2587\n"
     ]
    }
   ],
   "source": [
    "# Also save the candidate pool since the avaialble positive only cover 2/3 of the misconceptions\n",
    "train_misconceptions = df_mapping['MisconceptionName'].tolist()\n",
    "train_misconceptions = [{'text': x} for x in train_misconceptions if x]\n",
    "dump_json_or_pickle(train_misconceptions, '/tmp/candidate_pool.jsonl')\n",
    "print('Num misconceptions:', len(train_misconceptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Model Finetuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model finetuning process involves:\n",
    "\n",
    "1. Tokenization and length filtering using FLAGS embedding framework\n",
    "2. Training configuration:\n",
    "    - Learning rate: 1e-5\n",
    "    - Batch size: 32 \n",
    "    - Max sequence length: 128\n",
    "    - Warmup steps: 500\n",
    "    - Weight decay: 0.01\n",
    "    - Training epochs: 3\n",
    "3. Adding special tokens for retrieval optimization:\n",
    "    - Special token `<response>` added to sequence end\n",
    "    - Last token embedding used for similarity computation\n",
    "4. Training process:\n",
    "    - Compute cosine similarity between query and misconception embeddings\n",
    "    - Optimize embeddings through contrastive learning\n",
    "    - Early stopping based on validation performance\n",
    "    - Gradient clipping for stable training\n",
    "    \n",
    "Implementation note: Qwen models require special handling since they were not pre-trained for retrieval tasks. We add an extra token at sequence end and use its embedding for similarity matching during both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Hard Negative Mining Process\n",
    "\n",
    "The training process involes iterative hard negative mining to improve the model's performance. The process is as follows:\n",
    "\n",
    "| **Round** | **Stage**       | **Command** |\n",
    "|-----------|-----------------|------------|\n",
    "| **1**     | Hard Negative Mining | `scripts/mine_hn.sh BAAI/bge-base-en-v1.5  data/retriver/train_items.jsonl data/retriver/hn_r1.jsonl` |\n",
    "| **1**     | Training        | `scripts/train.sh data/retriver/hn_r1.jsonl outputs/models/0.5B/round1/` |\n",
    "| **2**     | Hard Negative Mining | `scripts/mine_hn.sh outputs/models/0.5B/round1/ data/retriver/train_items.jsonl data/retriver/hn_r2.jsonl` |\n",
    "| **2**     | Training        | `scripts/train.sh data/retriver/hn_r2.jsonl outputs/models/0.5B/round2/` |\n",
    "| **3**     | Hard Negative Mining | `scripts/mine_hn.sh outputs/models/0.5B/round2/ data/retriver/train_items.jsonl data/retriver/hn_r3.jsonl` |\n",
    "| **3**     | Training        | `scripts/train.sh data/retriver/hn_r3.jsonl outputs/models/0.5B/round3/` |\n",
    "\n",
    "\n",
    "### Notes:\n",
    "- **Hard Negative Mining:** Uses the previous round's trained model to mine hard negatives for better data.\n",
    "- **Training Parameters:** Learning rate, batch size, and other configurations are consistent across rounds.\n",
    "- **Iterations:** This process refines the model iteratively to improve its retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 3. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Service (app/embed_service_app.py)\n",
    "\n",
    "The Flask application serves the trained model as an embedding service with the following features:\n",
    "\n",
    "1. **Model Loading**\n",
    "    - Loads quantized model from checkpoint\n",
    "    - Configures model parameters and special tokens\n",
    "    - Handles both base model and LoRA weights\n",
    "\n",
    "2. **REST API**\n",
    "    - Endpoint: `/v1/embeddings`\n",
    "    - Method: POST\n",
    "    - Input: JSON with text array\n",
    "    - Output: Embedding vectors \n",
    "\n",
    "3. **Request Processing**\n",
    "    - Batches input text\n",
    "    - Applies tokenization and padding\n",
    "    - Returns embeddings in JSON format\n",
    "\n",
    "4. **Error Handling**\n",
    "    - Input validation\n",
    "    - Exception catching\n",
    "    - Informative error messages\n",
    "\n",
    "5. **Resource Management** \n",
    "    - Efficient batch processing\n",
    "    - GPU memory optimization\n",
    "    - Request queueing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python app/embed_service_app.py --base_model Qwen/Qwen2.5-0.5B --lora_path ./outputs/models/0.5B/round1/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_embeddings(texts, url = \"http://0.0.0.0:8080/v1/embeddings\"):\n",
    "     \"\"\"\n",
    "     Get embeddings for a list of texts from the local embedding service\n",
    "     \n",
    "     Args:\n",
    "          texts (list): List of strings to get embeddings for\n",
    "     \n",
    "     Returns:\n",
    "          dict: Response from the embedding service\n",
    "     \"\"\"\n",
    "     \n",
    "     payload = {\"texts\": texts}\n",
    "     response = requests.post(url, json=payload)\n",
    "     response =  response.json()\n",
    "     return np.array(response[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>CorrectAnswerText</th>\n",
       "      <th>Option</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>MisconceptionName</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>- **Question**: \\[\\n3 \\times 2+4-5\\n\\]\\nWhere ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId                                      ConstructName SubjectName  \\\n",
       "0           0  Use the order of operations to carry out calcu...      BIDMAS   \n",
       "\n",
       "  CorrectAnswer                                       QuestionText  \\\n",
       "0             A  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...   \n",
       "\n",
       "       CorrectAnswerText Option              AnswerText  MisconceptionId  \\\n",
       "0  \\( 3 \\times(2+4)-5 \\)      B  \\( 3 \\times 2+(4-5) \\)              NaN   \n",
       "\n",
       "  MisconceptionName                                             Prompt  \n",
       "0              None  - **Question**: \\[\\n3 \\times 2+4-5\\n\\]\\nWhere ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_flat.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f\"<instruct>{task_description}\\n<query>{query}\"\n",
    "\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f\"<instruct>{task_description}\\n<query>{query}\\n<response>{response}\"\n",
    "\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len\n",
    "        - len(tokenizer(\"<s>\", add_special_tokens=False)[\"input_ids\"])\n",
    "        - len(tokenizer(\"\\n<response></s>\", add_special_tokens=False)[\"input_ids\"]),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)[\"input_ids\"]\n",
    "    suffix_ids = tokenizer(\"\\n<response>\", add_special_tokens=False)[\"input_ids\"]\n",
    "    new_max_length = (\n",
    "        len(prefix_ids) + len(suffix_ids) + query_max_len + 8\n",
    "    ) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs[\"input_ids\"])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + \"\\n<response>\"\n",
    "    return new_max_length, new_queries\n",
    "\n",
    "\n",
    "df_val_flat = df_val_flat.dropna(subset=[\"MisconceptionId\"])\n",
    "task = \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [get_detailed_instruct(task, q) for q in df_val_flat[\"Prompt\"]]\n",
    "documents = df_mapping[\"MisconceptionName\"].tolist()\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "LORA_PATH = \"../models/kaggle-models/2211-lora-14b-transformers-default-v1/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "examples_prefix = \"\"\n",
    "new_query_max_len, new_queries = get_new_queries(\n",
    "    queries, query_max_len, examples_prefix, tokenizer\n",
    ")\n",
    "queries_embeddings = get_embeddings(new_queries)\n",
    "documents_embeddings = get_embeddings(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    q_embeds: torch.Tensor, d_embeds: torch.Tensor, target_ids: List[int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute MAP@25 and Recall@50 metrics.\n",
    "\n",
    "    Args:\n",
    "        q_embeds (torch.Tensor): Query embeddings of shape (M, dim), where M is the number of queries.\n",
    "        d_embeds (torch.Tensor): Document embeddings of shape (N, dim), where N is the number of documents.\n",
    "        target_ids (List[int]): List of target document indices (length M, one target index per query).\n",
    "\n",
    "    Returns:\n",
    "        None: Prints MAP@25 and Recall@100.\n",
    "    \"\"\"\n",
    "    # Compute similarity scores\n",
    "    scores = q_embeds @ d_embeds.T  # Shape: (M, N)\n",
    "\n",
    "    # Initialize variables for metrics\n",
    "    avg_precisions = []  # To store average precision for each query\n",
    "    recall_counts = []  # To store recall@100 counts for each query\n",
    "\n",
    "    # Compute metrics for each query\n",
    "    for i, target_id in enumerate(target_ids):\n",
    "        # Sort document indices by score in descending order\n",
    "        sorted_indices = torch.argsort(scores[i], descending=True)\n",
    "\n",
    "        relevant_docs = (sorted_indices[:500] == target_id).nonzero(as_tuple=True)[\n",
    "            0\n",
    "        ] \n",
    "        recall_count = (\n",
    "            1 if len(relevant_docs) > 0 else 0\n",
    "        )  # Check if target is in the top 100\n",
    "        recall_counts.append(recall_count)\n",
    "\n",
    "        # Compute average precision for top 25 (MAP@25)\n",
    "        precision_at_k = 0.0\n",
    "        num_relevant = 0\n",
    "        for rank, idx in enumerate(sorted_indices[:25]):\n",
    "            if idx == target_id:\n",
    "                num_relevant += 1\n",
    "                precision_at_k += num_relevant / (rank + 1)\n",
    "        avg_precisions.append(precision_at_k / 1 if num_relevant > 0 else 0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    map25 = sum(avg_precisions) / len(avg_precisions)\n",
    "    recall50 = sum(recall_counts) / len(recall_counts)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"MAP@25: {map25:.4f}\")\n",
    "    print(f\"Recall@50: {recall50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@25: 0.7561\n",
      "Recall@50: 1.0000\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(\n",
    "    torch.tensor(queries_embeddings),\n",
    "    torch.tensor(documents_embeddings),\n",
    "    df_val_flat[\"MisconceptionId\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Demo application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model quantization\n",
    "To further optimize the model for deployment, we quantize the model using AWQ. For best result, you should use an indomain dataset to calibrate the quantization.\n",
    "```bash\n",
    "!python scripts/convert_to_awq.py Qwen/Qwen2.5-0.5B \\\n",
    "    outputs/models/0.5B/round1/awq \\\n",
    "    --lora_path outputs/models/0.5B/round1/\n",
    "```\n",
    "```\n",
    "# # Expected output:\n",
    "\n",
    "# # Model already merged at outputs/models/0.5B/round1/awq/merged\n",
    "# # Quantized model will be saved at outputs/models/0.5B/round1/awq/awq\n",
    "# # Repo card metadata block was not found. Setting CardData to empty.\n",
    "# # AWQ: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [03:13<00:00,  8.07s/it]\n",
    "# # [2025-01-05 19:48:07,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
    "# # Model is quantized and saved at \"outputs/models/0.5B/round1/awq/awq\"\n",
    "# # ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
